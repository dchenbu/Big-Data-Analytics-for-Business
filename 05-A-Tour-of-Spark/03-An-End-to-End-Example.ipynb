{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# An End to End Example\n\nIn this section, we will reinforce everything we learned previously in this chapter with a more realistic example, and explain step by step what is happening under the hood. We\u2019ll use Spark to analyze some flight data from the United States Bureau of Transportation statistics.\n\nWe will start working with data/flight-data/csv/2015-summary.csv. First few rows of this file look like:\n\n```\n$ head data/flight-data/csv/2015-summary.csv\n\nDEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count\nUnited States,Romania,15\nUnited States,Croatia,1\nUnited States,Ireland,344\nEgypt,United States,15\n```\n\nFirst, we will read this file into Spark.\n\nNote: Replace the bucket name (`is843`) with your own bucket name and make sure that the data file can be found in Google Cloud Storage."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "datapath = \"gs://is843/notebooks/jupyter/data/flight-data/csv/2015-summary.csv\""}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "flightData2015 = spark.read.option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(datapath)"}, {"cell_type": "markdown", "metadata": {}, "source": "`inferSchema` is guessing the data types by reading a little bit of the data. In real life this has to be done in a more realiable way, but for the purpose of this example it would be fine.\n\nWe also want to specify that the first row is the header in the file, so we\u2019ll specify that as an option, too.\n\nReading data is a transformation, and is therefore a lazy operation. \n\nIf we perform the take action on the DataFrame, we will be able to see the same results that we saw before when we used the command line:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15)]"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "flightData2015.take(4)"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/04-03-DataFrame-take.png?raw=true\" width=\"700\" align=\"center\"/>\n\nNow, let\u2019s sort our data according to the `DEST_COUNTRY_NAME` column:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4),\n Row(DEST_COUNTRY_NAME='Angola', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='Anguilla', ORIGIN_COUNTRY_NAME='United States', count=41)]"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "flightData2015.sort(\"DEST_COUNTRY_NAME\").take(3)"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/04-03-DataFrame-sort-take.png?raw=true\" width=\"700\" align=\"center\"/>\n\nNothing happens to the data when we call sort because it\u2019s just a transformation. However, we can see that Spark is building up a plan for how it will execute this across the cluster by looking at the `explain()` plan. We can call explain on any DataFrame object to see the DataFrame\u2019s lineage (or how Spark will execute this query):"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [DEST_COUNTRY_NAME#16 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(DEST_COUNTRY_NAME#16 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#39]\n      +- FileScan csv [DEST_COUNTRY_NAME#16,ORIGIN_COUNTRY_NAME#17,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[gs://is843/notebooks/jupyter/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n\n\n"}], "source": "flightData2015.sort(\"DEST_COUNTRY_NAME\").explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "You can read explain plans from top to bottom, the top being the end result, and the bottom being the source(s) of data.\n\nIn this case, take a look at the first keywords. You will see sort, exchange, and FileScan. That\u2019s because the sort of our data is actually a wide transformation because rows will need to be compared with one another.\n\nBy default, when we perform a shuffle, Spark outputs 200 shuffle partitions. Let\u2019s set this value to 5 to reduce the number of the output partitions from the shuffle:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(DEST_COUNTRY_NAME='Algeria', ORIGIN_COUNTRY_NAME='United States', count=4),\n Row(DEST_COUNTRY_NAME='Angola', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='Anguilla', ORIGIN_COUNTRY_NAME='United States', count=41)]"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "flightData2015.sort(\"DEST_COUNTRY_NAME\").take(3)"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/04-03-DataFrame-partition.png?raw=true\" width=\"700\" align=\"center\"/>"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [DEST_COUNTRY_NAME#16 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(DEST_COUNTRY_NAME#16 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#55]\n      +- FileScan csv [DEST_COUNTRY_NAME#16,ORIGIN_COUNTRY_NAME#17,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[gs://is843/notebooks/jupyter/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n\n\n"}], "source": "flightData2015.sort(\"DEST_COUNTRY_NAME\").explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "From `explain()` we can confirm that we have 5 shuffle partitions. In experimenting with different values, you should see drastically different runtimes for larger datasets."}, {"cell_type": "markdown", "metadata": {}, "source": "## DataFrames and SQL\n\nSpark can run the same transformations, regardless of the language, in the exact same way. You can express your business logic in SQL or DataFrames (either in R, Python, Scala, or Java) and Spark will compile that logic down to an underlying plan (that you can see in the explain plan) before actually executing your code. With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. There is no performance difference between writing SQL queries or writing DataFrame code, they both \u201ccompile\u201d to the same underlying plan that we specify in DataFrame code.\n\nYou can make any DataFrame into a table or view with one simple method call:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Now we can query our data in SQL. To do so, we\u2019ll use the spark.sql function (remember, spark is our SparkSession variable) that conveniently returns a new DataFrame. Although this might seem a bit circular in logic\u2014that a SQL query against a DataFrame returns another DataFrame\u2014it\u2019s actually quite powerful. This makes it possible for you to specify transformations in the manner most convenient to you at any given point in time and not sacrifice any efficiency to do so! To understand that this is happening, let\u2019s take a look at two explain plans:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), ENSURE_REQUIREMENTS, [id=#71]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n         +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[gs://is843/notebooks/jupyter/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[count(1)])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), ENSURE_REQUIREMENTS, [id=#84]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_count(1)])\n         +- FileScan csv [DEST_COUNTRY_NAME#16] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[gs://is843/notebooks/jupyter/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n\n\n"}], "source": "sqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(*)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()\n\nsqlWay.explain()\ndataFrameWay.explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "**Notice that these plans compile to the exact same underlying plan!**\n\nLet\u2019s pull out some interesting statistics from our data. One thing to understand is that DataFrames and SQL in Spark already have a huge number of manipulations available. There are hundreds of functions that you can use and import to help you resolve your big data problems faster.\n\nLet's find the maximum number of flights to and from any given location in both DataFrame and SQL ways:"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(max(count)=370002)]"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(max(count)=370002)]"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import max\n\nflightData2015.select(max(\"count\")).take(1)"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's now find the top five destination countries in the data:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n"}], "source": "maxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n"}], "source": "from pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#111L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#16,destination_total#111L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[sum(cast(count#18 as bigint))])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#16, 5), ENSURE_REQUIREMENTS, [id=#270]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#16], functions=[partial_sum(cast(count#18 as bigint))])\n            +- FileScan csv [DEST_COUNTRY_NAME#16,count#18] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[gs://is843/notebooks/jupyter/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n\n\n"}], "source": "flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/04-03-DataFrame-transformation-flow.png?raw=true\" width=\"700\" align=\"center\"/>\n\nThe true execution plan (the one visible in explain) will differ from that shown in figure above because of optimizations in the physical execution, however, the illustration is as good starting point. This execution plan is a directed acyclic graph (DAG) of transformations, each resulting in a new immutable DataFrame, on which we call an action to generate a result."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}, "name": "02_A_Gentle_Introduction_to_Spark", "notebookId": 4108134220462460}, "nbformat": 4, "nbformat_minor": 4}